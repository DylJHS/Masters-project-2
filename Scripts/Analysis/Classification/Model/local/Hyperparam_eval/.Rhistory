metrics_plt <- data %>%
ggplot(aes(x = factor(Metric, levels = rev(c(
"Balanced Accuracy","F1", "Precision",
"Recall", "Specificity"))), y = Value, fill = Type)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
scale_fill_manual(values = c("#ABDDDE", "#CCEDB1", "#41B7C4")) +
facet_wrap(~Class, scales = "free_y") +
theme_minimal() +
labs(
title = paste0("Model Performance Stats for ", feature),
x = "Class",
y = "Value"
) +
scale_y_continuous(breaks = seq(0, 1, by = 0.25))+
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5),
panel.grid.major.x = element_line(colour = "grey",
linetype = 2),
panel.grid.major.y = element_blank(),
panel.grid.minor.x = element_blank()
)
# save the data
# write.csv(
#   confusion_matrix_data,
#   paste0(
#     "Data/Model_output/Confusion_Matrix_Stats_",
#     feature,
#     ".csv"
#   )
# )
#
# # save the plot
# ggsave(
#   filename = paste0(
#     "Plots/Model_Plots/General_model/Confusion_Matrix_Stats/",
#     feature,
#     "_Confusion_Matrix_Stats.png"
#   ),
#   plot = confusion_matrix_data,
#   width = 10,
#   height = 10
# )
}
# Assess the performance of the models
for (feature in response_features) {
predicted <- paste0("pred_", feature)
labelled <- paste0("act_", feature)
if (feature %in% cat_features) {
# Use Caret package to get the performance stats
cm_base <- calculate_confusion_matrix(final_base_oof_predictions, predicted, labelled)
cm_meta <- calculate_confusion_matrix(final_meta_oof_predictions, predicted, labelled)
# Create the TPR confusion matrix and the heatmap
# tpr_mtrx_base <- tpr_confusion_matrix(cm_base, feature, "Base")
# tpr_mtrx_meta <- tpr_confusion_matrix(cm_meta, feature, "Meta")
# Create the confusion matrix stats
confusion_matrix_data <- rbind(
create_confusion_matrix_data(cm_meta, "Meta"),
create_confusion_matrix_data(cm_base, "Base")
)
# Create the plot for the stats above
metrics_plot <- confusion_stat_plot(confusion_matrix_data, feature)
}
# else if (feature %in% reg_features) {
#   y <- as.numeric(base_oof_predictions[[paste0("act_", feature)]])
#   y_pred <- meta_oof_predictions[[paste0("pred_", feature)]]
#
#   cat(
#     "Feature: ", feature, "\n\n",
#     "RMSE: ", sqrt(mean((y - y_pred)^2)), "\n"
#   )
# } else {
#   cat("Feature not found")
# }
}
metrics_plot
# Load the functions
# Function to map factor levels to weights
feature_digit_function <- function(factors) {
sapply(factors, function(x) selected_weights[as.numeric(x)])
}
# Function to extract the fold indices from the folds
combine_all_folds <- function(folds) {
all_indices <- c()
for (fold in folds) {
all_indices <- c(all_indices, fold)
}
return(all_indices)
}
# Function to calculate the confusion matrix using the caret package and return the confusion matrix object
calculate_confusion_matrix <- function(predictions_df, predicted_col, true_col) {
# Ensure the input columns exist in the predictions dataframe
if (!predicted_col %in% names(predictions_df) || !true_col %in% names(predictions_df)) {
stop("Specified columns do not exist in the provided dataframe")
}
# Create the confusion matrix
cm <- confusionMatrix(
data = factor(predictions_df[[predicted_col]], levels = c(-1, 0, 1)),
reference = factor(predictions_df[[true_col]], levels = c(-1, 0, 1))
)
# Return the confusion matrix object
return(cm)
}
# Function to extract the true positive rate from the confusion matrix and create the heatmap
tpr_confusion_matrix <- function(object, feature, learner_type) {
new_table <- pivot_wider(as.data.frame(object$table), names_from = Reference, values_from = Freq, values_fill = list(frequency = 0)) %>%
column_to_rownames("Prediction") %>%
sweep(., 2, colSums(.), "/") %>%
round(., 2) %>%
rownames_to_column("Prediction") %>%
pivot_longer(., cols = -Prediction, names_to = "Reference", values_to = "TPR") %>%
mutate(TPR = round(TPR, 2))
graph <- ggplot(new_table, aes(x = Reference, y = Prediction, fill = TPR)) +
geom_tile() +
geom_text(aes(label = TPR), vjust = 1) +
scale_fill_gradient(low = "#FFEBEE", high = "#B71C1C") +
theme_minimal() +
labs(
title = paste0(
learner_type,
" Learner Performance for ",
feature),
x = "Actual Class",
y = "Predicted Class"
) +
theme(
axis.text.x = element_text(hjust = 1), # Adjust text angle for better legibility
plot.title = element_text(hjust = 0.5)
)
# Save the heatmap
ggsave(
filename = paste0(
"Plots/Model_Plots/General_model/Confusion_Matrices/",
feature,"_",
learner_type,
"_TPR_Heatmap.png"
),
plot = graph,
width = 10,
height = 10
)
}
# Function to get the stats from the confusion matrix object
create_confusion_matrix_data <- function(obj, type) {
as.data.frame(obj$byClass) %>%
rownames_to_column("Class") %>%
mutate(Type = type) %>%
select(-c("Pos Pred Value", "Neg Pred Value",
"Prevalence", "Detection Rate",
"Detection Prevalence", "Sensitivity")) %>%
pivot_longer(cols = -c(Type, Class), names_to = "Metric", values_to = "Value") %>%
as.data.frame()
}
# Function to create the plot based on the confusion matrix stats by learner type
confusion_stat_plot <- function(data, feature) {
metrics_plt <- data %>%
ggplot(aes(x = factor(Metric, levels = rev(c(
"Balanced Accuracy","F1", "Precision",
"Recall", "Specificity"))), y = Value, fill = Type)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
scale_fill_manual(values = c("#ABDDDE", "#CCEDB1", "#41B7C4")) +
facet_wrap(~Class) +
theme_minimal() +
labs(
title = paste0("Model Performance Stats for ", feature),
x = "Class",
y = "Value"
) +
scale_y_continuous(breaks = seq(0, 1, by = 0.25))+
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5),
panel.grid.major.x = element_line(colour = "grey",
linetype = 2),
panel.grid.major.y = element_blank(),
panel.grid.minor.x = element_blank()
)
# save the data
# write.csv(
#   confusion_matrix_data,
#   paste0(
#     "Data/Model_output/Confusion_Matrix_Stats_",
#     feature,
#     ".csv"
#   )
# )
#
# # save the plot
# ggsave(
#   filename = paste0(
#     "Plots/Model_Plots/General_model/Confusion_Matrix_Stats/",
#     feature,
#     "_Confusion_Matrix_Stats.png"
#   ),
#   plot = confusion_matrix_data,
#   width = 10,
#   height = 10
# )
}
# Assess the performance of the models
for (feature in response_features) {
predicted <- paste0("pred_", feature)
labelled <- paste0("act_", feature)
if (feature %in% cat_features) {
# Use Caret package to get the performance stats
cm_base <- calculate_confusion_matrix(final_base_oof_predictions, predicted, labelled)
cm_meta <- calculate_confusion_matrix(final_meta_oof_predictions, predicted, labelled)
# Create the TPR confusion matrix and the heatmap
# tpr_mtrx_base <- tpr_confusion_matrix(cm_base, feature, "Base")
# tpr_mtrx_meta <- tpr_confusion_matrix(cm_meta, feature, "Meta")
# Create the confusion matrix stats
confusion_matrix_data <- rbind(
create_confusion_matrix_data(cm_meta, "Meta"),
create_confusion_matrix_data(cm_base, "Base")
)
# Create the plot for the stats above
metrics_plot <- confusion_stat_plot(confusion_matrix_data, feature)
}
# else if (feature %in% reg_features) {
#   y <- as.numeric(base_oof_predictions[[paste0("act_", feature)]])
#   y_pred <- meta_oof_predictions[[paste0("pred_", feature)]]
#
#   cat(
#     "Feature: ", feature, "\n\n",
#     "RMSE: ", sqrt(mean((y - y_pred)^2)), "\n"
#   )
# } else {
#   cat("Feature not found")
# }
}
metrics_plot
# Load the functions
# Function to map factor levels to weights
feature_digit_function <- function(factors) {
sapply(factors, function(x) selected_weights[as.numeric(x)])
}
# Function to extract the fold indices from the folds
combine_all_folds <- function(folds) {
all_indices <- c()
for (fold in folds) {
all_indices <- c(all_indices, fold)
}
return(all_indices)
}
# Function to calculate the confusion matrix using the caret package and return the confusion matrix object
calculate_confusion_matrix <- function(predictions_df, predicted_col, true_col) {
# Ensure the input columns exist in the predictions dataframe
if (!predicted_col %in% names(predictions_df) || !true_col %in% names(predictions_df)) {
stop("Specified columns do not exist in the provided dataframe")
}
# Create the confusion matrix
cm <- confusionMatrix(
data = factor(predictions_df[[predicted_col]], levels = c(-1, 0, 1)),
reference = factor(predictions_df[[true_col]], levels = c(-1, 0, 1))
)
# Return the confusion matrix object
return(cm)
}
# Function to extract the true positive rate from the confusion matrix and create the heatmap
tpr_confusion_matrix <- function(object, feature, learner_type) {
new_table <- pivot_wider(as.data.frame(object$table), names_from = Reference, values_from = Freq, values_fill = list(frequency = 0)) %>%
column_to_rownames("Prediction") %>%
sweep(., 2, colSums(.), "/") %>%
round(., 2) %>%
rownames_to_column("Prediction") %>%
pivot_longer(., cols = -Prediction, names_to = "Reference", values_to = "TPR") %>%
mutate(TPR = round(TPR, 2))
graph <- ggplot(new_table, aes(x = Reference, y = Prediction, fill = TPR)) +
geom_tile() +
geom_text(aes(label = TPR), vjust = 1) +
scale_fill_gradient(low = "#FFEBEE", high = "#B71C1C") +
theme_minimal() +
labs(
title = paste0(
learner_type,
" Learner Performance for ",
feature),
x = "Actual Class",
y = "Predicted Class"
) +
theme(
axis.text.x = element_text(hjust = 1), # Adjust text angle for better legibility
plot.title = element_text(hjust = 0.5)
)
# Save the heatmap
ggsave(
filename = paste0(
"Plots/Model_Plots/General_model/Confusion_Matrices/",
feature,"_",
learner_type,
"_TPR_Heatmap.png"
),
plot = graph,
width = 10,
height = 10
)
}
# Function to get the stats from the confusion matrix object
create_confusion_matrix_data <- function(obj, type) {
as.data.frame(obj$byClass) %>%
rownames_to_column("Class") %>%
mutate(Type = type) %>%
select(-c("Pos Pred Value", "Neg Pred Value",
"Prevalence", "Detection Rate",
"Detection Prevalence", "Sensitivity")) %>%
pivot_longer(cols = -c(Type, Class), names_to = "Metric", values_to = "Value") %>%
as.data.frame()
}
# Function to create the plot based on the confusion matrix stats by learner type
confusion_stat_plot <- function(data, feature) {
metrics_plt <- data %>%
ggplot(aes(x = factor(Metric, levels = rev(c(
"Balanced Accuracy","F1", "Precision",
"Recall", "Specificity"))), y = Value, fill = Type)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
scale_fill_manual(values = c("#ABDDDE", "#CCEDB1", "#41B7C4")) +
facet_wrap(~Class, scales = "free_y") +
theme_minimal() +
labs(
title = paste0("Model Performance Stats for ", feature),
x = "Class",
y = "Value"
) +
scale_y_continuous(breaks = seq(0, 1, by = 0.25))+
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5),
panel.grid.major.x = element_line(colour = "grey",
linetype = 2),
panel.grid.major.y = element_blank(),
panel.grid.minor.x = element_blank()
)
# save the data
# write.csv(
#   confusion_matrix_data,
#   paste0(
#     "Data/Model_output/Confusion_Matrix_Stats_",
#     feature,
#     ".csv"
#   )
# )
#
# # save the plot
# ggsave(
#   filename = paste0(
#     "Plots/Model_Plots/General_model/Confusion_Matrix_Stats/",
#     feature,
#     "_Confusion_Matrix_Stats.png"
#   ),
#   plot = confusion_matrix_data,
#   width = 10,
#   height = 10
# )
}
# Load the functions
# Function to map factor levels to weights
feature_digit_function <- function(factors) {
sapply(factors, function(x) selected_weights[as.numeric(x)])
}
# Function to extract the fold indices from the folds
combine_all_folds <- function(folds) {
all_indices <- c()
for (fold in folds) {
all_indices <- c(all_indices, fold)
}
return(all_indices)
}
# Function to calculate the confusion matrix using the caret package and return the confusion matrix object
calculate_confusion_matrix <- function(predictions_df, predicted_col, true_col) {
# Ensure the input columns exist in the predictions dataframe
if (!predicted_col %in% names(predictions_df) || !true_col %in% names(predictions_df)) {
stop("Specified columns do not exist in the provided dataframe")
}
# Create the confusion matrix
cm <- confusionMatrix(
data = factor(predictions_df[[predicted_col]], levels = c(-1, 0, 1)),
reference = factor(predictions_df[[true_col]], levels = c(-1, 0, 1))
)
# Return the confusion matrix object
return(cm)
}
# Function to extract the true positive rate from the confusion matrix and create the heatmap
tpr_confusion_matrix <- function(object, feature, learner_type) {
new_table <- pivot_wider(as.data.frame(object$table), names_from = Reference, values_from = Freq, values_fill = list(frequency = 0)) %>%
column_to_rownames("Prediction") %>%
sweep(., 2, colSums(.), "/") %>%
round(., 2) %>%
rownames_to_column("Prediction") %>%
pivot_longer(., cols = -Prediction, names_to = "Reference", values_to = "TPR") %>%
mutate(TPR = round(TPR, 2))
graph <- ggplot(new_table, aes(x = Reference, y = Prediction, fill = TPR)) +
geom_tile() +
geom_text(aes(label = TPR), vjust = 1) +
scale_fill_gradient(low = "#FFEBEE", high = "#B71C1C") +
theme_minimal() +
labs(
title = paste0(
learner_type,
" Learner Performance for ",
feature),
x = "Actual Class",
y = "Predicted Class"
) +
theme(
axis.text.x = element_text(hjust = 1), # Adjust text angle for better legibility
plot.title = element_text(hjust = 0.5)
)
# Save the heatmap
ggsave(
filename = paste0(
"Plots/Model_Plots/General_model/Confusion_Matrices/",
feature,"_",
learner_type,
"_TPR_Heatmap.png"
),
plot = graph,
width = 10,
height = 10
)
}
# Function to get the stats from the confusion matrix object
create_confusion_matrix_data <- function(obj, type) {
as.data.frame(obj$byClass) %>%
rownames_to_column("Class") %>%
mutate(Type = type) %>%
select(-c("Pos Pred Value", "Neg Pred Value",
"Prevalence", "Detection Rate",
"Detection Prevalence", "Sensitivity")) %>%
pivot_longer(cols = -c(Type, Class), names_to = "Metric", values_to = "Value") %>%
as.data.frame()
}
# Function to create the plot based on the confusion matrix stats by learner type
confusion_stat_plot <- function(data, feature) {
metrics_plt <- data %>%
ggplot(aes(x = factor(Metric, levels = rev(c(
"Balanced Accuracy","F1", "Precision",
"Recall", "Specificity"))), y = Value, fill = Type)) +
geom_bar(stat = "identity", position = "dodge") +
coord_flip() +
scale_fill_manual(values = c("#ABDDDE", "#CCEDB1", "#41B7C4")) +
facet_wrap(~Class, scales = "free_y") +
theme_minimal() +
labs(
title = paste0("Model Performance Stats for ", feature),
x = "Class",
y = "Value"
) +
scale_y_continuous(breaks = seq(0, 1, by = 0.25))+
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(hjust = 0.5),
panel.grid.major.x = element_line(colour = "grey",
linetype = 2),
panel.grid.major.y = element_blank(),
panel.grid.minor.x = element_blank()
)
# save the data
# write.csv(
#   confusion_matrix_data,
#   paste0(
#     "Data/Model_output/Confusion_Matrix_Stats_",
#     feature,
#     ".csv"
#   )
# )
#
# # save the plot
# ggsave(
#   filename = paste0(
#     "Plots/Model_Plots/General_model/Confusion_Matrix_Stats/",
#     feature,
#     "_Confusion_Matrix_Stats.png"
#   ),
#   plot = confusion_matrix_data,
#   width = 10,
#   height = 10
# )
}
# Assess the performance of the models
for (feature in response_features) {
predicted <- paste0("pred_", feature)
labelled <- paste0("act_", feature)
if (feature %in% cat_features) {
# Use Caret package to get the performance stats
cm_base <- calculate_confusion_matrix(final_base_oof_predictions, predicted, labelled)
cm_meta <- calculate_confusion_matrix(final_meta_oof_predictions, predicted, labelled)
# Create the TPR confusion matrix and the heatmap
# tpr_mtrx_base <- tpr_confusion_matrix(cm_base, feature, "Base")
# tpr_mtrx_meta <- tpr_confusion_matrix(cm_meta, feature, "Meta")
# Create the confusion matrix stats
confusion_matrix_data <- rbind(
create_confusion_matrix_data(cm_meta, "Meta"),
create_confusion_matrix_data(cm_base, "Base")
)
# Create the plot for the stats above
metrics_plot <- confusion_stat_plot(confusion_matrix_data, feature)
}
# else if (feature %in% reg_features) {
#   y <- as.numeric(base_oof_predictions[[paste0("act_", feature)]])
#   y_pred <- meta_oof_predictions[[paste0("pred_", feature)]]
#
#   cat(
#     "Feature: ", feature, "\n\n",
#     "RMSE: ", sqrt(mean((y - y_pred)^2)), "\n"
#   )
# } else {
#   cat("Feature not found")
# }
}

---
title: "XGB full prediction"
output: html_document
date: "2024-06-27"
note: "This file is intended to be used for the full prediction of the XGB model 
from the soi RNAseq data to the final predictions from the meta learners"
---
```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "/Users/Dyll/Documents/Education/VU_UVA/Internship/Epigenetics/Janssen_Group-UMCUtrecht/Main_Project")
```


```{r}
# Load the packages
library(dplyr)
library(readr)
library(tidyverse)
library(xgboost)
library(caret)
library(ggplot2)
```


```{r}
print_every <- 100
early_stop <- 25


# setwd("/Users/Dyll/Documents/Education/VU_UVA/Internship/Epigenetics/Janssen_Group-UMCUtrecht/Main_Project")

input_path <- "Data/Gen_model_input/"
```

FUNCTIONS
```{r}
# Load the functions
# Function to map factor levels to weights
feature_digit_function <- function(factors, reference) {
  sapply(factors, function(x) reference[as.numeric(x)])
}

# Function to extract the fold indices from the folds
combine_all_folds <- function(folds) {
  all_indices <- c()
  for (fold in folds) {
    all_indices <- c(all_indices, fold)
  }
  return(all_indices)
}

# Function to calculate the confusion matrix using the caret package and return the confusion matrix object
calculate_confusion_matrix <- function(predictions_df, predicted_col, true_col) {
  # Ensure the input columns exist in the predictions dataframe
  if (!predicted_col %in% names(predictions_df) || !true_col %in% names(predictions_df)) {
    stop("Specified columns do not exist in the provided dataframe")
  }

  # Create the confusion matrix
  cm <- confusionMatrix(
    data = factor(predictions_df[[predicted_col]], levels = c(-1, 0, 1)),
    reference = factor(predictions_df[[true_col]], levels = c(-1, 0, 1))
  )

  # Return the confusion matrix object
  return(cm)
}

# Function to extract the true positive rate from the confusion matrix and create the heatmap
tpr_confusion_matrix <- function(object, feature, learner_type) {
  new_table <- pivot_wider(as.data.frame(object$table), names_from = Reference, values_from = Freq, values_fill = list(frequency = 0)) %>%
    column_to_rownames("Prediction") %>%
    sweep(., 2, colSums(.), "/") %>%
    round(., 2) %>%
    rownames_to_column("Prediction") %>%
    pivot_longer(., cols = -Prediction, names_to = "Reference", values_to = "TPR") %>%
    mutate(TPR = round(TPR, 2))

  graph <- ggplot(new_table, aes(x = Reference, y = Prediction, fill = TPR)) +
    geom_tile() +
    geom_text(aes(label = TPR), vjust = 1) +
    scale_fill_gradient(
      low = "#FFEBEE", high = "#B71C1C",
      limits = c(0, 1)
    ) +
    theme_minimal() +
    labs(
      title = paste0(
        learner_type,
        " Learner Performance for ",
        feature
      ),
      x = "Actual Class",
      y = "Predicted Class"
    ) +
    theme(
      axis.text.x = element_text(hjust = 1), # Adjust text angle for better legibility
      plot.title = element_text(hjust = 0.5),
      legend.title = element_blank()
    )

  # save the plot
  filefold <- file.path("Plots/Model_Plots/General_model/Results/Categorical/Confusion_matrices")
  if (!dir.exists(filefold)) {
    dir.create(filefold, recursive = TRUE)
  }

  # Save the heatmap
  ggsave(
    filename = paste0(
      filefold,
      feature, "_",
      learner_type,
      "_TPR_Heatmap.png"
    ),
    plot = graph,
    width = 10,
    height = 10
  )
}

# Function to get the stats from the confusion matrix object
create_confusion_matrix_data <- function(obj, type) {
  as.data.frame(obj$byClass) %>%
    rownames_to_column("Class") %>%
    mutate(Type = type) %>%
    select(-c(
      "Pos Pred Value", "Neg Pred Value",
      "Prevalence", "Detection Rate",
      "Detection Prevalence", "Sensitivity"
    )) %>%
    pivot_longer(cols = -c(Type, Class), names_to = "Metric", values_to = "Value") %>%
    as.data.frame()
}


# Function to create the plot based on the confusion matrix stats by learner type
confusion_stat_plot <- function(data) {
  Feature <- data$Feature[1]

  metrics_plt <- data %>%
    ggplot(aes(x = factor(Metric, levels = rev(c(
      "Balanced Accuracy", "F1", "Precision",
      "Recall", "Specificity"
    ))), y = Value, fill = Type)) +
    geom_bar(stat = "identity", position = "dodge") +
    coord_flip() +
    scale_fill_manual(values = c("#ABDDDE", "#CCEDB1", "#41B7C4")) +
    facet_wrap(~Class) +
    theme_minimal() +
    labs(
      title = paste0("Model Performance Stats for ", Feature),
      x = "Class",
      y = "Value"
    ) +
    scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      axis.text.y = element_text(size = 10, face = "bold"),
      plot.title = element_text(hjust = 0.5),
      panel.grid.major.x = element_line(
        colour = "grey",
        linetype = 2
      ),
      panel.grid.major.y = element_blank(),
      panel.grid.minor.x = element_blank(),
      panel.spacing = unit(4, "lines")
    )
  
  # save the plot
  filefold <- file.path("Plots/Model_Plots/General_model/Results/Categorical/Stat_metrics")
  if (!dir.exists(filefold)) {
    dir.create(filefold, recursive = TRUE)
  }
  
  ggsave(
    filename = file.path(filefold, paste0(
      Feature,
      "_Stats.pdf"
    )),
    plot = metrics_plt,
    width = 10,
    height = 10
  )
}


# Feature importance function to create the df and sort based on the Gain and plot the top 10 features 
feat_imp <- function(imp_df, top_gene_num = 10, basis = "Gain", Type) {
  # Create the feature importance matrix
  created_imp <- imp_df %>%
    group_by(Feature) %>%
    summarise_all(mean) %>%
    # create the combined normalised importance
    mutate(
      Combined = rowSums(across(.cols = -Feature))
    ) %>%
    mutate(
      Normalised = Combined / sum(Combined)
    ) %>%
    select(-Combined) %>%
    arrange(desc(basis))
  
  type_file_folder <- ifelse(Type == "Meta", "Meta_feat_imp/", "Base_feat_imp/")
  csv_filefold <- file.path("Data/Gen_model_output/Results/Feature_importance/",
                            type_file_folder)
  if (!dir.exists(csv_filefold)) {
    csv_filefold  }
  # Save the feature importance df
  write.csv(
    created_imp,
    paste0(
      csv_filefold,
      feature,
      "_feature_importance.csv"
    )
  )
  
  
  # Create the plot for the top 10 features
  top_genes <- created_imp %>%
    select(Feature, basis) %>%
    top_n(top_gene_num, basis) %>%
    arrange(desc(basis))
  
  # Create the plot
  top_genes_plot <- ggplot(top_genes[1:top_gene_num,],
                           aes(x = reorder(Feature, .data[[basis]]), 
                               y = .data[[basis]])
                           ) +
    geom_bar(stat = "identity", fill = "steelblue") +
    labs(
      title = paste0("Top ", top_gene_num, " Features for ", feature),
      x = "Feature",
      y = basis
    ) +
    coord_flip() +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      plot.title = element_text(hjust = 0.5)
    )

  # Save the plot
  plt_filefold <- file.path("Plots/Model_Plots/General_model/Results/Feature_importance/",
                        type_file_folder)
  if (!dir.exists(plt_filefold)) {
    dir.create(plt_filefold, recursive = TRUE)
  }

  ggsave(
    filename = paste0(
      plt_filefold,
      feature,
      "_Top_",
      top_gene_num,
      "_Features.pdf"
    ),
    plot = top_genes_plot,
    width = 10,
    height = 10
  )
  
  return(list(feature_imp_df = created_imp, top_genes = top_genes))
}
```

RNA SETS
```{r}
# Set the constant variables
# The RNA list
rna_list <- list(
  transcripts_per_million = "tpm",
  scaled_transcripts_per_million = "scld_tpm",
  log_scaled_transcripts_per_million = "log_scld_tpm",
  log_transcripts_per_million = "log_tpm",
  expected_counts = "exp",
  scaled_expected_counts = "scld",
  log_expected_counts = "log",
  log_scaled_expected_counts = "log_scld"
)
```

RESPONSE FEATURES
```{r}
# The CIN response features
# Categorical features
cat_cin <- read_tsv(
  paste0(
    input_path,
    "CIN_Features/PANCAN_ArmCallsAndAneuploidyScore_092817.txt"
  ),
  show_col_types = FALSE
) %>%
  replace(is.na(.), 0) %>%
  select(-c("Type", "Aneuploidy Score")) %>%
  mutate(Sample = str_replace_all(Sample, "-", "\\.")) %>%
  column_to_rownames("Sample") %>%
  mutate_all(~ replace(., . == 1, 2)) %>%
  mutate_all(~ replace(., . == 0, 1)) %>%
  mutate_all(~ replace(., . == -1, 0)) %>%
  rename(
    "13q" = "13 (13q)",
    "14q" = "14 (14q)",
    "15q" = "15 (15q)",
    "21q" = "21 (21q)",
    "22q" = "22 (22q)"
  )

cat("\n Categorical CIN:  \n")
print(head(cat_cin[20:30]))

# Numerical features
# HRD scores
ori_hrd <- read_tsv(
  paste0(input_path, "CIN_Features/TCGA.HRD_withSampleID.txt"),
  show_col_types = FALSE
)

t_hrd <- as.data.frame(t(ori_hrd))
first_hrd <- t_hrd
colnames(first_hrd) <- t_hrd[1, ]
hrd <- as.data.frame(first_hrd[-1, ]) %>%
  mutate_all(as.numeric) %>%
  rename(loh_hrd = "hrd-loh") %>%
  select(-HRD) %>%
  mutate(new = str_replace_all(rownames(.), "-", "\\."))

rm(first_hrd)
rm(ori_hrd)
rm(t_hrd)

rownames(hrd) <- hrd$new
hrd <- hrd %>%
  select(-new)

# print(head(hrd))

# Pericentromeric CNVs
peri_cnv <- read.csv(
  paste0(
    input_path,
    "/CIN_Features/lim_alpha_incl_TCGA_pericentro.csv"
  )
) %>%
  mutate_all(~ replace(., is.na(.), 0)) %>%
  mutate(sampleID = gsub("-", ".", sampleID)) %>%
  column_to_rownames("sampleID")

# print(dim(peri_cnv))
```

HYPERPARAMETERS
```{r}
base_cat_parameters <- read.csv(
  paste0(
    input_path,
    "Hyperparameters/base_cat_hyperparams.csv"
  )
)
base_reg_parameters <- read.csv(
  paste0(
    input_path,
    "Hyperparameters/base_reg_hyperparams.csv"
  )
)

meta_parameters <- read.csv(
  paste0(
    input_path,
    "Hyperparameters/meta_hyperparams.csv"
  )
)
```

FULL CIN DF
```{r}
reg_cin <- merge(
  hrd,
  peri_cnv,
  by = "row.names"
) %>%
  mutate(Row.names = str_replace_all(Row.names, "-", ".")) %>%
  column_to_rownames("Row.names")
cat("\n Regression CIN:  \n")
print(head(reg_cin[1:5]))

# rm(peri_cnv)
# rm(hrd)

full_cin <- merge(
  cat_cin,
  reg_cin,
  by = "row.names"
) %>%
  column_to_rownames("Row.names")
cat("\n Full CIN:  \n")
print(head(full_cin[1:5]))

# All response feature names
response_features <- colnames(full_cin)
reg_features <- colnames(reg_cin)
cat_features <- colnames(cat_cin)
cat(
  "\n Response Features # :", length(response_features), "\n",
  "Regression Features # :", length(reg_features), "\n",
  "Categorical Features # :", length(cat_features), "\n"
)

# rm(reg_cin)
# rm(cat_cin)
```

FOLDS
```{r}
# Create the folds to be used

# Create the full data by merging the soi & cancerous RNA data with the CIN data
full_data <- merge(
  read.csv(
    paste0(
      input_path,
      "RNA/Train/train_tpm_soi.csv"
    ),
    row.names = 1
  ),
  full_cin,
  by = "row.names"
)

cat("\n Full Data:  \n")
print(head(full_data[1:5]))

# Creating the folds and returning the indices for the out-of-fold predictions only
folds <- createFolds(full_data[["1p"]], k = 2, list = TRUE, returnTrain = FALSE)
cat("\n Folds # :", length(folds), "\n")

# Create the empty dataframe that will store the out-of-fold predictions for each model
base_oof_predictions <- data.frame(
  matrix(
    ncol = length(response_features),
    nrow = length(combine_all_folds(folds))
  )
)
colnames(base_oof_predictions) <- paste0("pred_", response_features)

cat("\n Predictions Dataframe:  \n")
print(head(base_oof_predictions[1:5]))
```

LABELS
```{r}
# Get the actual (true) fold labels
labels <- full_data %>%
  select(all_of(intersect(response_features, colnames(.)))) %>%
  mutate(
    index = as.factor(row_number())
  ) %>%
  rename_with(~ paste0("act_", .))

# merge the fold labels with the empty predictions dataframe
# making sure to keep the order of the original indices
base_oof_predictions <- cbind(base_oof_predictions, labels) %>%
  arrange(act_index)
```

BASE TRAINING
```{r}
# Create the full feature importance dataframe
full_feature_imp <- data.frame()

for (feature in response_features[32:48]) {
  cat(
    "Feature: ", feature, "\n\n"
  )
  
  # Create df to store the importance matrix
  feature_imp_df <- data.frame()
  
  # Create the out-of-fold predictions column
  oof_predictions <- numeric(nrow(base_oof_predictions))
  
  # Determine parameter set based on feature type
  parameters <- if (feature %in% cat_features) {
    base_cat_parameters
  } else if (feature %in% reg_features) {
    base_reg_parameters
  } else {
    cat("Feature not found")
    next
  }
  
  # Select the parameters and weights corresponding to the feature
  selected_parameters <- parameters[parameters$Feature == feature, ]
  
  if (nrow(selected_parameters) == 0) {
    cat("No parameters found for feature: ", feature, "\n")
    next
  }
  
  # Print selected parameters
  cat("\n Selected Parameters:\n")
  print(selected_parameters)
  
  # Load and prepare data
  rna_selection_name <- rna_list[[selected_parameters$RNA_set]]
  rna_set <- read.csv(paste0(
    input_path, 
    "RNA/Train/train_", 
    rna_selection_name, 
    "_soi.csv"), 
    row.names = 1
  )
  
  full_df <- merge(rna_set, full_cin, by = "row.names")
  
  y <- ifelse(feature %in% cat_features, as.integer, as.numeric)(full_df[[feature]])
  # cat("\n y: ")
  # print(head(y))
  
  X <- full_df %>% select(-c("Row.names", all_of(response_features)))
  
  # Initialize containers for predictions and feature importance
  oof_predictions <- numeric(nrow(X))
  feature_imp_df <- data.frame()
  
  # Fold-wise training
  for (fold_index in seq_along(folds)) {
    cat(
      "\n Fold: ", fold_index, "\n"
    )
    train_indices <- setdiff(seq_len(nrow(full_df)), folds[[fold_index]])
    valid_indices <- folds[[fold_index]]
    
    train_data <- xgb.DMatrix(data = as.matrix(X[train_indices, ]), label = y[train_indices])
    valid_data <- xgb.DMatrix(data = as.matrix(X[valid_indices, ]), label = y[valid_indices])
    
    # Adjust weights for categorical features
    if (feature %in% cat_features) {
      selected_ref <- as.numeric(
      selected_parameters[c(
        "Weight_loss",
        "Weight_normal",
        "Weight_gain"
      )])
      weights <- as.numeric(feature_digit_function(factor(y[train_indices], levels = c(0, 1, 2)), selected_ref))
      setinfo(train_data, "weight", weights)
    }
    
    # Train model
    params_list <- list(
      data = train_data,
      nrounds = selected_parameters$Trees,
      objective = ifelse(feature %in% cat_features, "multi:softmax", "reg:squarederror"),
      eval_metric = ifelse(feature %in% cat_features, "mlogloss", "rmse"),
      max_depth = selected_parameters$Max_depth,
      min_child_weight = selected_parameters$Child_weight,
      early_stopping_rounds = early_stop,
      eta = selected_parameters$Eta,
      gamma = selected_parameters$Gamma,
      watchlist = list(eval = valid_data),
      print_every_n = print_every
    )
    
    # Only add num_class for categorical features
    if (feature %in% cat_features) {
      params_list$num_class <- 3}
    
    xgb_model <- do.call(xgb.train, params_list)
    
    # Store OOF predictions
    oof_predictions[valid_indices] <- predict(xgb_model, valid_data)
    cat("\n OOF Predictions: \n",
        dim(oof_predictions), "\n")
    print(head(oof_predictions, 10))
    
    # Create the feature importance matrix
    importance_matrix <- xgb.importance(feature_names = colnames(X), model = xgb_model)
    
    # Store the importance matrix in the dataframe
    feature_imp_df <- rbind(feature_imp_df, as.data.frame(importance_matrix))
  }
  
  # Store the predictions in the corresponding column
  base_oof_predictions[[paste0("pred_", feature)]] <- oof_predictions
  print(base_oof_predictions[[paste0("pred_", feature)]][1:10])
  
  # get the feature imp df
  imp <- feat_imp(imp_df = feature_imp_df)
  
  top_genes <- imp$top_genes
  
  full_feature_imp <- rbind(
    full_feature_imp, 
    imp$feature_imp_df %>%
      mutate(Target = feature)
  ) %>% 
    arrange(desc(Gain))
  
  }
# # Save the predictions
# write.csv(
#   base_oof_predictions,
#   "Data/Gen_model_output/Predictions/Base_predictions/Full_base_predictions.csv"
# )
# write.csv(
#   base_oof_predictions,
#   "Data/Gen_model_input/Base_predictions/Full_base_predictions.csv"
# )

cat("\n\n Training the base models complete")
```
General Feature Importance Plot
```{r}
# Plot of the top feature for each target
top_gene_each <- full_feature_imp %>%
  group_by(Target) %>%
  top_n(2, Gain) %>%
  arrange(desc(Gain))

top_barplt <- ggplot(top_gene_each, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Top Feature for each Target",
    x = "Feature",
    y = "Gain"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(hjust = 0.5)
  ) +
  geom_text(aes(label = Target ), vjust = 1.5 , size = 3)

print(top_barplt)

# Heatmap of the top features 
top_genes_complete <- full_feature_imp %>%
  filter(Feature %in% top_gene_each$Feature) %>%
  select(Feature, Target, Gain) %>%
  pivot_wider(names_from = Feature, values_from = Gain) %>% 
  replace(is.na(.), 0) %>% 
  column_to_rownames("Target")

pheatmap(top_genes_complete, cluster_rows = FALSE, cluster_cols = TRUE, border_color = "black", show_colnames = TRUE, show_rownames = TRUE, main = "Top Target Features", )


```


META MODEL INITIALIZATION
```{r}
# Upload the base predictions
base_oof_predictions <- read.csv(
  paste0(
    input_path,
    "Base_predictions/Full_base_predictions.csv"
  ),
  row.names = 1
)

# Create the training folds
meta_folds <- createFolds(base_oof_predictions[["act_1p"]], k = 5, list = TRUE, returnTrain = FALSE)

# Create the empty dataframe that will store the out-of-fold predictions for each model of the meta learners
meta_oof_predictions <- data.frame(
  matrix(
    ncol = length(response_features),
    nrow = length(combine_all_folds(meta_folds))
  )
)
colnames(meta_oof_predictions) <- paste0("pred_", response_features)

cat("\n Predictions Dataframe:  \n")
print(head(meta_oof_predictions[1:5]))

meta_oof_predictions <- cbind(
  meta_oof_predictions,
  labels
) %>%
  arrange(act_index) %>%
  column_to_rownames("act_index")
```

META TRAINING
```{r}
# Create the predictor data for the meta learners
meta_input_data <- base_oof_predictions %>%
  select(starts_with("pred_"))

# Create the full feature importance dataframe
full_feature_imp_df <- data.frame()

for (feature in response_features) {
  cat(
    "Feature: ", feature, "\n\n"
  )

  # Get the parameters from stored Parameters file
  parameters <- meta_parameters

  # select the parameters and weights corresponding to the index
  selected_parameters <- parameters[parameters$Feature == feature, ]



  y <- as.integer(base_oof_predictions[[paste0("act_", feature)]])
  X <- meta_input_data

  # Create the out-of-fold predictions column
  oof_predictions <- numeric(nrow(X))

  # Create df to store the importance matrix
  feature_imp_df <- data.frame()
  if (feature %in% cat_features) {

    # Set the weights
    selected_weights <- base_cat_parameters[base_cat_parameters$Feature == feature, c("Weight_loss", "Weight_normal", "Weight_gain")]

    # Fold-wise training
    for (fold_index in seq_along(folds)) {
      cat(
        "Fold: ", fold_index, "\n"
      )
      train_indices <- setdiff(seq_len(nrow(full_df)), folds[[fold_index]])
      valid_indices <- folds[[fold_index]]

      train_data <- xgb.DMatrix(data = as.matrix(X[train_indices, ]), label = y[train_indices])
      valid_data <- xgb.DMatrix(data = as.matrix(X[valid_indices, ]), label = y[valid_indices])

      train_y_factor <- factor(y[train_indices], levels = c(0, 1, 2))
      weights <- as.numeric(feature_digit_function(train_y_factor))
      setinfo(train_data, "weight", weights)

      xgb_model <- xgb.train(
        data = train_data,
        nrounds = selected_trees,
        objective = "multi:softmax",
        eval_metric = "mlogloss",
        max_depth = selected_max_depth,
        min_child_weight = selected_min_child,
        early_stopping_rounds = early_stop,
        eta = selected_eta,
        gamma = selected_gamma,
        watchlist = list(eval = valid_data),
        num_class = 3,
        print_every_n = print_every,
      )

      # Store OOF predictions
      oof_predictions[valid_indices] <- predict(xgb_model, valid_data)

      # Create the feature importance matrix
      importance_matrix <- xgb.importance(feature_names = colnames(X), model = xgb_model)

      # Store the importance matrix in the dataframe
      feature_imp_df <- rbind(feature_imp_df, as.data.frame(importance_matrix))
    }

    # Store the predictions in the corresponding column
    meta_oof_predictions[[paste0("pred_", selected_feature)]] <- oof_predictions

  } else if (feature %in% reg_features) {

    xgb_meta_model <- xgb.cv(
      data = xgb_data,
      nrounds = selected_trees,
      objective = "reg:squarederror",
      eval_metric = "rmse",
      early_stopping_rounds = early_stop,
      folds = meta_folds,
      max_depth = selected_max_depth,
      min_child_weight = selected_min_child,
      eta = selected_eta,
      gamma = selected_gamma,
      print_every_n = print_every,
      prediction = TRUE
    )

    # Store the predictions in the corresponding column
    meta_oof_predictions[[paste0("pred_", selected_feature)]] <- xgb_meta_model$pred
  } else {
    cat("Feature not found")
  }
  
  # Modify the feature importance matrix
  feature_imp_df <- feature_imp_df %>%
    group_by(Feature) %>%
    summarise_all(mean) %>%
    # create the combined normalised importance
    mutate(
      Combined = rowSums(across(.cols = -Feature))
    ) %>%
    mutate(
      Normalised = Combined / sum(Combined),
      Target = feature
    ) %>%
    arrange(desc(Gain))
}

cat("Training of the meta models complete")

# Save the out-of-fold predictions
write.csv(
  meta_oof_predictions,
  "Data/Gen_model_output/Predictions/Meta_predictions/Full_meta_predictions.csv"
)
write.csv(
  meta_oof_predictions,
  "Data/Gen_model_input/Meta_predictions/Full_meta_predictions.csv"
)
```

Reconvert the data into the original scale
```{r}
# Upload the meta predictions
meta_oof_predictions <- read.csv(
  paste0(
    input_path,
    "Meta_predictions/Full_meta_predictions.csv"
  ),
  row.names = 1
)

# columns to not convert
regression_columns <- c(
  paste("pred_", reg_features, sep = ""),
  paste("act_", reg_features, sep = "")
)

# Reconvert the data into the original scale
final_base_oof_predictions <- base_oof_predictions %>%
  mutate(across(.cols = -SampleID, .fns = as.numeric)) %>%
  mutate(across(.cols = -c("SampleID", regression_columns), ~ replace(., . == 0, -1))) %>%
  mutate(across(.cols = -c("SampleID", regression_columns), ~ replace(., . == 1, 0))) %>%
  mutate(across(.cols = -c("SampleID", regression_columns), ~ replace(., . == 2, 1)))

final_meta_oof_predictions <- meta_oof_predictions %>%
  mutate_all(as.numeric) %>%
  mutate(across(.cols = -regression_columns, ~ replace(., . == 0, -1))) %>%
  mutate(across(.cols = -regression_columns, ~ replace(., . == 1, 0))) %>%
  mutate(across(.cols = -regression_columns, ~ replace(., . == 2, 1)))
```


Assess the performance of the models
```{r}
Class_metrics_df <- data.frame(
  Feature = character(),
  Class = character(),
  Type = character(),
  Metric = character(),
  Value = numeric()
)

Cat_general_metrics <- data.frame(
  Feature = character(),
  Type = character(),
  Accuracy = numeric(),
  Kappa = numeric(),
  AccuracyNull = numeric(),
  AccuracyPValue = numeric()
)

Reg_general_metrics <- data.frame(
  Feature = character(),
  Type = character(),
  RMSE = numeric(),
  MAE = numeric()
)

# Assess the performance of the models
for (feature in response_features) {
  predicted <- paste0("pred_", feature)
  labelled <- paste0("act_", feature)

  if (feature %in% cat_features) { # Categorical features
    # Use Caret package to get the performance stats
    cm_base <- calculate_confusion_matrix(final_base_oof_predictions, predicted, labelled)
    cm_meta <- calculate_confusion_matrix(final_meta_oof_predictions, predicted, labelled)

    # Update the general metrics dataframeß
    cat_base_metrics <- as.data.frame(cm_base$overall) %>%
      t() %>%
      as.data.frame() %>%
      mutate(Feature = feature, Type = "Base") %>%
      select(Feature, Type, everything())

    cat_meta_metrics <- as.data.frame(cm_meta$overall) %>%
      t() %>%
      as.data.frame() %>%
      mutate(Feature = feature, Type = "Meta") %>%
      select(Feature, Type, everything())

    Cat_general_metrics <- rbind(
      Cat_general_metrics,
      cat_base_metrics,
      cat_meta_metrics
    )

    # # Create the TPR confusion matrix and the heatmap
    # tpr_mtrx_base <- tpr_confusion_matrix(cm_base, feature, "Base")
    # tpr_mtrx_meta <- tpr_confusion_matrix(cm_meta, feature, "Meta")

    # Create the confusion matrix stats
    confusion_matrix_data <- rbind(
      create_confusion_matrix_data(cm_meta, "Meta"),
      create_confusion_matrix_data(cm_base, "Base")
    ) %>%
      mutate(Feature = feature) %>%
      select(Feature, everything())

    # Update the class metrics dataframe
    Class_metrics_df <- rbind(Class_metrics_df, confusion_matrix_data)

    # Create the plot for the stats above
    metrics_plot <- confusion_stat_plot(confusion_matrix_data)
  } else if (feature %in% reg_features) { # Regression models
    # evaluate the performance of the regression models with caret

    # Calculate the RMSE for the regression models
    rmse_base <- caret::RMSE(
      final_base_oof_predictions[[predicted]],
      final_base_oof_predictions[[labelled]]
    )
    rmse_meta <- caret::RMSE(
      final_meta_oof_predictions[[predicted]],
      final_meta_oof_predictions[[labelled]]
    )

    # Calulate the MAE for the regression models
    mae_base <- caret::MAE(
      final_base_oof_predictions[[predicted]],
      final_base_oof_predictions[[labelled]]
    )
    mae_meta <- caret::MAE(
      final_meta_oof_predictions[[predicted]],
      final_meta_oof_predictions[[labelled]]
    )

    # Calculate the R2 for the regression models
    r2_base <- caret::R2(
      final_base_oof_predictions[[predicted]],
      final_base_oof_predictions[[labelled]]
    ) %>% round(., 2)

    r2_meta <- caret::R2(
      final_meta_oof_predictions[[predicted]],
      final_meta_oof_predictions[[labelled]]
    ) %>% round(., 2)


    # Update the general metrics dataframe
    Reg_general_metrics <- rbind(
      Reg_general_metrics,
      data.frame(
        Feature = feature,
        Type = "Base",
        RMSE = rmse_base,
        MAE = mae_base,
        R_squared = r2_base
      ),
      data.frame(
        Feature = feature,
        Type = "Meta",
        RMSE = rmse_meta,
        MAE = mae_meta,
        R_squared = r2_meta
      )
    )

    # Create the actual vs predicted plot f
    # Function to create a predicted vs actual plot
    create_pred_vs_actual_plot <- function(data, feature, type, plot_title_prefix = "Predicted vs Actual for ") {
      ggplot(data, aes_string(
        x = paste0("pred_", feature), # Predicted on x-axis
        y = paste0("act_", feature) # Actual on y-axis
      )) +
        geom_point() +
        geom_smooth(method = "lm", se = FALSE, color = "blue") +
        labs(
          title = paste0(plot_title_prefix, str_replace_all(feature, "_", " ")),
          x = "Predicted",
          y = "Actual",
          caption = paste0("Linear Regression R² = ", get(paste0("r2_", type)))
        ) +
        theme_minimal() +
        theme(
          plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
          axis.title = element_text(size = 14),
          panel.border = element_rect(linetype = "solid", color = "black", fill = NA, size = 0.5),
          panel.grid.major = element_line(color = "gray20", size = 0.2),
          panel.grid.minor = element_line(color = "gray85", size = 0.1)
        )
    }

    plot_base_reg <- create_pred_vs_actual_plot(
      final_base_oof_predictions,
      feature,
      "base",
      "Base model residuals for "
    )

    plot_meta_reg <- create_pred_vs_actual_plot(
      final_meta_oof_predictions,
      feature,
      "meta",
      "Meta model residuals for "
    )

    print(plot_meta_reg)
  }
}

# Reset the indices of the dataframes to row number
Cat_general_metrics <- Cat_general_metrics %>%
  rownames_to_column("Index") %>%
  select(-Index) %>%
  distinct() %>%
  select(Feature, Type, Accuracy, Kappa, AccuracyNull, AccuracyPValue)

Reg_general_metrics <- Reg_general_metrics %>%
  rownames_to_column("Index") %>%
  select(-Index) %>%
  distinct()

Class_metrics_df <- Class_metrics_df %>%
  distinct()

# Save the dataframes
write.csv(
  Cat_general_metrics,
  "Data/Gen_model_output/Results/Gen_cat_metrics.csv"
)

write.csv(
  Reg_general_metrics,
  "Data/Gen_model_output/Results/Gen_reg_metrics.csv"
)

write.csv(
  Class_metrics_df,
  "Data/Gen_model_output/Results/Gen_cat_class_metrics.csv"
)
```




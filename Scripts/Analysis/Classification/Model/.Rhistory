Depth = numeric(),
Learning_Rate = numeric(),
Gamma = numeric(),
Loss = numeric()
)
set.seed(33)
for (i in 1:length(rna_list)){
rna <- rna_list[[i]]
name <- names(rna_list)[i]
full_df <- merge(rna, full_cin, by = "row.names")
sample_split <- sample.split(Y = full_df[colnames(full_cin)], SplitRatio = 0.7)
train_set <- subset(x = full_df, sample_split == TRUE)
test_set <- subset(x = full_df, sample_split == FALSE)
cat(paste0("\n", name, ": \n"))
for (feature in aneu_feature_list){
cat(paste0("\n", feature, ": \n"))
y_train <- as.integer(train_set[[feature]])
X_train <- train_set %>% select(-append("Row.names",colnames(full_cin)))
y_test <- as.integer(test_set[[feature]])
X_test <- test_set %>% select(-append("Row.names",colnames(full_cin)))
y_train[y_train == -1] <- 0
y_train[y_train == 1] <- 2
y_train[y_train == 0] <- 1
y_test[y_test == -1] <- 0
y_test[y_test == 1] <- 2
y_test[y_test == 0] <- 1
xgb_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
xgb_test <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)
watchlist <- list(train = xgb_train, test = xgb_test)
for (depth in seq(2,3,4)){
cat(paste0("\t Depth: ", depth, "\n"))
for (gam in seq(0,3,0.5)) {
cat(paste0("\t\t Gamma: ", gam, "\n"))
xgb_params <- list(
booster = "gbtree",
num_class=3,
max_depth = depth,
gamma = gam,
subsample = 0.75,
colsample_bytree = 1,
objective = "multi:softmax",
eval_metric = "mlogloss",
eta = 0.04 # Seems like the best eta is the closest one to 0
)
xgb_model <- xgb.train(
params = xgb_params,
data = xgb_train,
watchlist = watchlist,
early_stopping_rounds = 50,
nrounds = 6000,
verbose = 0
)
# Best loss and iteration number
best_loss <- min(xgb_model[["evaluation_log"]][["test_mlogloss"]])
xgb_cat_preds <- predict(xgb_model, as.matrix(X_test), reshape = TRUE)
xgb_cat_preds <- as.data.frame(xgb_cat_preds)
xgb_cat_preds <- xgb_cat_preds %>%
rename(pred = "xgb_cat_preds") %>%
mutate(mod_pred = ifelse(pred == 0, -1, ifelse(pred == 1, 0, 1)),
actual = y_test,
residual = round(pred - actual, 3),
res_sqrd = residual^2
)
aneu_cat_metrics_df <- rbind(aneu_cat_metrics_df,
data.frame(
RNA_Set = name,
Feature = feature,
Depth = depth,
Learning_Rate = lr,
Gamma = gam,
Loss = round(
sqrt(
sum(
xgb_cat_preds$res_sqrd)/
length(xgb_cat_preds$res_sqrd
)
),3
)
)
)
}
}
}
}
min_metrics <- aneu_cat_metrics_df %>%
group_by(Feature) %>%
summarise(Min = min(Loss)) %>%
left_join(aneu_cat_metrics_df, by = c("Feature", "Min" = "Loss"))
write.csv(aneu_cat_metrics_df, paste0("Data/aneu_cat_xgb_metrics_params_",Sys.Date(), ".csv"))
rna_list <- list(
transcripts_per_million = tpm_set, # Seems to be the best performing
scalled_transcripts_per_million = scld_tpm_set, # not too useful (scalled)
# log_scalled_transcripts_per_million = log_scld_tpm,
log_transcripts_per_million = log_tpm, # Best for Aneuploidy Score
# expected_counts =exp_set,
scalled_expected_counts = scld_exp_set, # not too useful (scalled)
log_expected_counts = log_exp,
log_scalled_expected_counts = log_scld_exp # not too useful (scalled)
)
feature_list <- colnames(full_cin[1,9:15])
aneu_reg_metrics_df <- data.frame(
RNA_Set = character(),
Feature = character(),
Depth = numeric(),
Learning_Rate = numeric(),
Gamma = numeric(),
RSME = numeric(),
MAE = numeric()
)
set.seed(35)
lr = 0.04
for (i in 1:length(rna_list)){
rna <- rna_list[[i]]
name <- names(rna_list)[i]
full_df <- merge(rna, full_cin, by = "row.names")
sample_split <- sample.split(Y = full_df[colnames(full_cin)], SplitRatio = 0.7)
train_set <- subset(x = full_df, sample_split == TRUE)
test_set <- subset(x = full_df, sample_split == FALSE)
cat(paste0("\n", name, ": \n"))
for (feature in feature_list){
cat(paste0("\n", feature, ": \n"))
y_train <- as.integer(train_set[[feature]])
X_train <- train_set %>% select(-append("Row.names",colnames(full_cin)))
y_test <- as.integer(test_set[[feature]])
X_test <- test_set %>% select(-append("Row.names",colnames(full_cin)))
xgb_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
xgb_test <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)
watchlist <- list(train = xgb_train, test = xgb_test)
for (depth in seq(2,4)){
cat(paste0("\t Depth: ", depth, "\n"))
# cat(paste0("\t\t Learning Rate: ", 0.04, "\n"))
for (gam in seq(0,1.25,0.25)){
cat(paste0("\t\t Gamma: ", gam))
xgb_params <- list(
booster = "gbtree",
max_depth = depth,
gamma = gam,
subsample = 0.75,
colsample_bytree = 1,
objective = "reg:squarederror",
eval_metric = "rmse",
eta = lr # Seems like the best eta is the closest one to 0
)
xgb_model <- xgb.train(
params = xgb_params,
data = xgb_train,
watchlist = watchlist,
early_stopping_rounds = 50,
nrounds = 6000,
verbose = 0
)
# xgb_model
# Best RMSE and iteration number
best_rmse <- min(xgb_model[["evaluation_log"]][["test_rmse"]])
xgb_preds <- predict(xgb_model, as.matrix(X_test), reshape = TRUE)
xgb_preds <- as.data.frame(xgb_preds)
xgb_preds <- xgb_preds %>%
rename(base_pred = "xgb_preds") %>%
mutate(mod_pred = round(base_pred),
actual = y_test,
base_residual = round(base_pred - actual, 3),
mod_residual = round(mod_pred - actual, 3),
base_res_sqrd = base_residual^2,
mod_res_sqrd = mod_residual^2
)
# assign(paste0("df_", variable), df)
base_rsme <- round(sqrt(
sum(xgb_preds$base_res_sqrd)/length(xgb_preds$base_res_sqrd)
),3)
mod_rsme <- round(sqrt(
sum(xgb_preds$mod_res_sqrd)/length(xgb_preds$mod_res_sqrd)
),3)
base_mae <- round(mean(abs(xgb_preds$base_residual)),3)
mod_mae <- round(mean(abs(xgb_preds$mod_residual)),3)
# Print the best RMSE and its corresponding iteration
# cat(paste(
# ": Best RMSE:", best_rmse, "at iteration:", best_iteration, '\n',
# "\t\t\t Modified RSME: ", mod_rsme, "\n",
# "\t\t\t Modified MAE: ", mod_mae, "\n"))
aneu_reg_metrics_df <- rbind(aneu_reg_metrics_df,
data.frame(
RNA_Set = name,
Feature = feature,
Depth = depth,
Learning_Rate = lr,
Gamma = gam,
RSME = base_rsme,
MAE = base_mae
)
)
}
}
}
}
min_metrics <- aneu_reg_metrics_df %>%
group_by(Feature) %>%
summarise(Min = min(RSME)) %>%
left_join(aneu_reg_metrics_df, by = c("Feature", "Min" = "RSME"))
write.csv(aneu_reg_metrics_df, paste0("Data/aneu_reg_xgb_metrics_params_",Sys.Date(), ".csv"))
rna_list <- list(
transcripts_per_million = tpm_set, # Seems to be the best performing
# scalled_transcripts_per_million = scld_tpm_set, # not too useful (scalled)
log_scalled_transcripts_per_million = log_scld_tpm,
# log_transcripts_per_million = log_tpm,
expected_counts = exp_set,
# scalled_expected_counts = scld_exp_set,
log_expected_counts = log_exp,
log_scalled_expected_counts = log_scld_exp
)
# aneu_feature_list <- colnames(full_cin[1,6:length(full_cin)])
aneu_feature_list <- colnames(full_cin[1,6:13])
aneu_cat_metrics_df <- data.frame(
RNA_Set = character(),
Feature = character(),
Depth = numeric(),
Learning_Rate = numeric(),
Gamma = numeric(),
Loss = numeric()
)
set.seed(33)
for (i in 1:length(rna_list)){
rna <- rna_list[[i]]
name <- names(rna_list)[i]
full_df <- merge(rna, full_cin, by = "row.names")
sample_split <- sample.split(Y = full_df[colnames(full_cin)], SplitRatio = 0.7)
train_set <- subset(x = full_df, sample_split == TRUE)
test_set <- subset(x = full_df, sample_split == FALSE)
cat(paste0("\n\n", name, ":"))
for (feature in aneu_feature_list){
cat(paste0("\n\t", feature, ":"))
y_train <- as.integer(train_set[[feature]])
X_train <- train_set %>% select(-append("Row.names",colnames(full_cin)))
y_test <- as.integer(test_set[[feature]])
X_test <- test_set %>% select(-append("Row.names",colnames(full_cin)))
y_train[y_train == -1] <- 0
y_train[y_train == 1] <- 2
y_train[y_train == 0] <- 1
y_test[y_test == -1] <- 0
y_test[y_test == 1] <- 2
y_test[y_test == 0] <- 1
xgb_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
xgb_test <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)
watchlist <- list(train = xgb_train, test = xgb_test)
for (depth in seq(2,4)){
cat(paste0("\n\t\t Depth: ", depth, "\n"))
for (gam in seq(0,2.5,0.5)) {
cat(paste0("\t\t\t Gamma: ", gam))
xgb_params <- list(
booster = "gbtree",
num_class=3,
max_depth = depth,
gamma = gam,
subsample = 0.75,
colsample_bytree = 1,
objective = "multi:softmax",
eval_metric = "mlogloss",
eta = 0.04 # Seems like the best eta is the closest one to 0
)
xgb_model <- xgb.train(
params = xgb_params,
data = xgb_train,
watchlist = watchlist,
early_stopping_rounds = 50,
nrounds = 6000,
verbose = 0
)
# Best loss and iteration number
best_loss <- min(xgb_model[["evaluation_log"]][["test_mlogloss"]])
xgb_cat_preds <- predict(xgb_model, as.matrix(X_test), reshape = TRUE)
xgb_cat_preds <- as.data.frame(xgb_cat_preds)
xgb_cat_preds <- xgb_cat_preds %>%
rename(pred = "xgb_cat_preds") %>%
mutate(mod_pred = ifelse(pred == 0, -1, ifelse(pred == 1, 0, 1)),
actual = y_test,
residual = round(pred - actual, 3),
res_sqrd = residual^2
)
aneu_cat_metrics_df <- rbind(aneu_cat_metrics_df,
data.frame(
RNA_Set = name,
Feature = feature,
Depth = depth,
Learning_Rate = lr,
Gamma = gam,
Loss = round(
sqrt(
sum(
xgb_cat_preds$res_sqrd)/
length(xgb_cat_preds$res_sqrd
)
),3
)
)
)
}
}
}
}
rna_list <- list(
transcripts_per_million = tpm_set, # Seems to be the best performing
# scalled_transcripts_per_million = scld_tpm_set, # not too useful (scalled)
log_scalled_transcripts_per_million = log_scld_tpm,
# log_transcripts_per_million = log_tpm,
expected_counts = exp_set,
# scalled_expected_counts = scld_exp_set,
log_expected_counts = log_exp,
log_scalled_expected_counts = log_scld_exp
)
# aneu_feature_list <- colnames(full_cin[1,6:length(full_cin)])
aneu_feature_list <- colnames(full_cin[1,6:13])
aneu_cat_metrics_df <- data.frame(
RNA_Set = character(),
Feature = character(),
Depth = numeric(),
Learning_Rate = numeric(),
Gamma = numeric(),
Loss = numeric()
)
set.seed(33)
for (i in 1:length(rna_list)){
rna <- rna_list[[i]]
name <- names(rna_list)[i]
full_df <- merge(rna, full_cin, by = "row.names")
sample_split <- sample.split(Y = full_df[colnames(full_cin)], SplitRatio = 0.7)
train_set <- subset(x = full_df, sample_split == TRUE)
test_set <- subset(x = full_df, sample_split == FALSE)
cat(paste0("\n\n", name, ":"))
for (feature in aneu_feature_list){
cat(paste0("\n\t", feature, ":"))
y_train <- as.integer(train_set[[feature]])
X_train <- train_set %>% select(-append("Row.names",colnames(full_cin)))
y_test <- as.integer(test_set[[feature]])
X_test <- test_set %>% select(-append("Row.names",colnames(full_cin)))
y_train[y_train == -1] <- 0
y_train[y_train == 1] <- 2
y_train[y_train == 0] <- 1
y_test[y_test == -1] <- 0
y_test[y_test == 1] <- 2
y_test[y_test == 0] <- 1
xgb_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
xgb_test <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)
watchlist <- list(train = xgb_train, test = xgb_test)
for (depth in seq(2,4)){
cat(paste0("\n\t\t Depth: ", depth, "\n"))
for (gam in seq(0.25,2.5,0.5)) {
cat(paste0("\t\t\t Gamma: ", gam))
xgb_params <- list(
booster = "gbtree",
num_class=3,
max_depth = depth,
gamma = gam,
subsample = 0.75,
colsample_bytree = 1,
objective = "multi:softmax",
eval_metric = "mlogloss",
eta = 0.04 # Seems like the best eta is the closest one to 0
)
xgb_model <- xgb.train(
params = xgb_params,
data = xgb_train,
watchlist = watchlist,
early_stopping_rounds = 50,
nrounds = 6000,
verbose = 0
)
# Best loss and iteration number
best_loss <- min(xgb_model[["evaluation_log"]][["test_mlogloss"]])
xgb_cat_preds <- predict(xgb_model, as.matrix(X_test), reshape = TRUE)
xgb_cat_preds <- as.data.frame(xgb_cat_preds)
xgb_cat_preds <- xgb_cat_preds %>%
rename(pred = "xgb_cat_preds") %>%
mutate(mod_pred = ifelse(pred == 0, -1, ifelse(pred == 1, 0, 1)),
actual = y_test,
residual = round(pred - actual, 3),
res_sqrd = residual^2
)
aneu_cat_metrics_df <- rbind(aneu_cat_metrics_df,
data.frame(
RNA_Set = name,
Feature = feature,
Depth = depth,
Learning_Rate = lr,
Gamma = gam,
Loss = round(
sqrt(
sum(
xgb_cat_preds$res_sqrd)/
length(xgb_cat_preds$res_sqrd
)
),3
)
)
)
}
}
}
}
min_metrics <- aneu_cat_metrics_df %>%
group_by(Feature) %>%
summarise(Min = min(Loss)) %>%
left_join(aneu_cat_metrics_df, by = c("Feature", "Min" = "Loss"))
write.csv(aneu_cat_metrics_df, paste0("Data/aneu_cat_xgb_metrics_params_",Sys.Date(), ".csv"))
rna_list <- list(
transcripts_per_million = tpm_set, # Seems to be the best performing
scalled_transcripts_per_million = scld_tpm_set, # not too useful (scalled)
# log_scalled_transcripts_per_million = log_scld_tpm,
log_transcripts_per_million = log_tpm, # Best for Aneuploidy Score
# expected_counts =exp_set,
scalled_expected_counts = scld_exp_set, # not too useful (scalled)
log_expected_counts = log_exp,
log_scalled_expected_counts = log_scld_exp # not too useful (scalled)
)
feature_list <- colnames(full_cin[1,6:15])
aneu_reg_metrics_df <- data.frame(
RNA_Set = character(),
Feature = character(),
Depth = numeric(),
Learning_Rate = numeric(),
Gamma = numeric(),
RSME = numeric(),
MAE = numeric()
)
set.seed(35)
lr = 0.04
for (i in 1:length(rna_list)){
rna <- rna_list[[i]]
name <- names(rna_list)[i]
full_df <- merge(rna, full_cin, by = "row.names")
sample_split <- sample.split(Y = full_df[colnames(full_cin)], SplitRatio = 0.7)
train_set <- subset(x = full_df, sample_split == TRUE)
test_set <- subset(x = full_df, sample_split == FALSE)
cat(paste0("\n", name, ": \n"))
for (feature in feature_list){
cat(paste0("\n\t", feature, ":"))
y_train <- as.integer(train_set[[feature]])
X_train <- train_set %>% select(-append("Row.names",colnames(full_cin)))
y_test <- as.integer(test_set[[feature]])
X_test <- test_set %>% select(-append("Row.names",colnames(full_cin)))
xgb_train <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
xgb_test <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)
watchlist <- list(train = xgb_train, test = xgb_test)
for (depth in seq(4,5)){
cat(paste0("\n\t\t Depth: ", depth, "\n"))
# cat(paste0("\t\t Learning Rate: ", 0.04, "\n"))
for (gam in seq(0.25,2.5,0.5)){
cat(paste0("\t\t\t Gamma: ", gam))
xgb_params <- list(
booster = "gbtree",
max_depth = depth,
gamma = gam,
subsample = 0.75,
colsample_bytree = 1,
objective = "reg:squarederror",
eval_metric = "rmse",
eta = lr # Seems like the best eta is the closest one to 0
)
xgb_model <- xgb.train(
params = xgb_params,
data = xgb_train,
watchlist = watchlist,
early_stopping_rounds = 50,
nrounds = 6000,
verbose = 0
)
# xgb_model
# Best RMSE and iteration number
best_rmse <- min(xgb_model[["evaluation_log"]][["test_rmse"]])
xgb_preds <- predict(xgb_model, as.matrix(X_test), reshape = TRUE)
xgb_preds <- as.data.frame(xgb_preds)
xgb_preds <- xgb_preds %>%
rename(base_pred = "xgb_preds") %>%
mutate(mod_pred = round(base_pred),
actual = y_test,
base_residual = round(base_pred - actual, 3),
mod_residual = round(mod_pred - actual, 3),
base_res_sqrd = base_residual^2,
mod_res_sqrd = mod_residual^2
)
# assign(paste0("df_", variable), df)
base_rsme <- round(sqrt(
sum(xgb_preds$base_res_sqrd)/length(xgb_preds$base_res_sqrd)
),3)
mod_rsme <- round(sqrt(
sum(xgb_preds$mod_res_sqrd)/length(xgb_preds$mod_res_sqrd)
),3)
base_mae <- round(mean(abs(xgb_preds$base_residual)),3)
mod_mae <- round(mean(abs(xgb_preds$mod_residual)),3)
# Print the best RMSE and its corresponding iteration
# cat(paste(
# ": Best RMSE:", best_rmse, "at iteration:", best_iteration, '\n',
# "\t\t\t Modified RSME: ", mod_rsme, "\n",
# "\t\t\t Modified MAE: ", mod_mae, "\n"))
aneu_reg_metrics_df <- rbind(aneu_reg_metrics_df,
data.frame(
RNA_Set = name,
Feature = feature,
Depth = depth,
Learning_Rate = lr,
Gamma = gam,
RSME = base_rsme,
MAE = base_mae
)
)
}
}
}
}
min_metrics <- aneu_reg_metrics_df %>%
group_by(Feature) %>%
summarise(Min = min(RSME)) %>%
left_join(aneu_reg_metrics_df, by = c("Feature", "Min" = "RSME"))
write.csv(aneu_reg_metrics_df, paste0("Data/aneu_reg_xgb_metrics_params_",Sys.Date(), ".csv"))
